# COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY
# SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>
# Obviously that is going to taint the analysis. But it also serves as a
# convenient marker to divide that long, long, long string into separate documents.
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# This left me with a list of 218 documents. On further inspection,
# some of them appeared to be a little on the short side (in my limited experience,
# the bard is not known for brevity). As it turns out, the short documents were
# the dramatis personae for his plays. I removed them as well.
(dramatis.personae <- grep("Dramatis Personae", shakespeare, ignore.case = TRUE))
shakespeare = shakespeare[-dramatis.personae]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# Down to 182 documents, each of which is a complete work.
# The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(shakespeare)
doc.corpus <- Corpus(doc.vec)
inspect(doc.corpus)
length(doc.corpus)
str(doc.corpus)
summary(doc.corpus)
summary(doc.corpus)
meta(doc.corpus[[1]])
# http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/
rm(list=ls())
library(tm)
library(SnowballC)
library(slam)
library(reshape2)
library(ggplot2)
TEXTFILE = "d:/data/pg100.txt"
if (!file.exists(TEXTFILE)) {
download.file("http://www.gutenberg.org/cache/epub/100/pg100.txt", destfile = TEXTFILE)
}
shakespeare = readLines(TEXTFILE)
length(shakespeare)
head(shakespeare)
# There seems to be some header and footer text. We will want to get rid of that!
# Using a text editor I checked to see how many lines were occupied with metadata
# and then removed them before concatenating all of the lines into a single long, long, long string.
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
nchar(shakespeare)
# While I had the text open in the editor I noticed that sections in the
# document were separated by the following text:
# <<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM
# SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS
# PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE
# WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE
# DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS
# PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED
# COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY
# SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>
# Obviously that is going to taint the analysis. But it also serves as a
# convenient marker to divide that long, long, long string into separate documents.
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# This left me with a list of 218 documents. On further inspection,
# some of them appeared to be a little on the short side (in my limited experience,
# the bard is not known for brevity). As it turns out, the short documents were
# the dramatis personae for his plays. I removed them as well.
(dramatis.personae <- grep("Dramatis Personae", shakespeare, ignore.case = TRUE))
shakespeare = shakespeare[-dramatis.personae]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# Down to 182 documents, each of which is a complete work.
# The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(shakespeare)
doc.corpus <- Corpus(doc.vec)
inspect(doc.corpus)
length(doc.corpus)
str(doc.corpus)
summary(doc.corpus)
meta(doc.corpus[[1]])
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
meta(doc.corpus[[1]])
doc.corpus <- tm_map(doc.corpus, PlainTextDocument)
meta(doc.corpus[[1]])
# http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/
rm(list=ls())
library(tm)
library(SnowballC)
library(slam)
library(reshape2)
library(ggplot2)
TEXTFILE = "d:/data/pg100.txt"
if (!file.exists(TEXTFILE)) {
download.file("http://www.gutenberg.org/cache/epub/100/pg100.txt", destfile = TEXTFILE)
}
shakespeare = readLines(TEXTFILE)
length(shakespeare)
head(shakespeare)
# There seems to be some header and footer text. We will want to get rid of that!
# Using a text editor I checked to see how many lines were occupied with metadata
# and then removed them before concatenating all of the lines into a single long, long, long string.
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
nchar(shakespeare)
# While I had the text open in the editor I noticed that sections in the
# document were separated by the following text:
# <<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM
# SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS
# PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE
# WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE
# DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS
# PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED
# COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY
# SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>
# Obviously that is going to taint the analysis. But it also serves as a
# convenient marker to divide that long, long, long string into separate documents.
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# This left me with a list of 218 documents. On further inspection,
# some of them appeared to be a little on the short side (in my limited experience,
# the bard is not known for brevity). As it turns out, the short documents were
# the dramatis personae for his plays. I removed them as well.
(dramatis.personae <- grep("Dramatis Personae", shakespeare, ignore.case = TRUE))
shakespeare = shakespeare[-dramatis.personae]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# Down to 182 documents, each of which is a complete work.
# The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(shakespeare)
doc.corpus <- Corpus(doc.vec)
inspect(doc.corpus)
length(doc.corpus)
str(doc.corpus)
summary(doc.corpus)
meta(doc.corpus[[1]])
# A corpus with 182 text documents
# The metadata consists of 2 tag-value pairs and a data frame
# Available tags are:
#   create_date creator
# Available variables in the data frame are:
#   MetaID
# There is a lot of information in those documents which is not particularly
# useful for text mining. So before proceeding any further, we will clean
# things up a bit. First we convert all of the text to lowercase and then
# remove punctuation, numbers and common English stopwords. Possibly the
# list of English stop words is not entirely appropriate for Shakespearean
# English, but it is a reasonable starting point.
doc.corpus <- tm_map(doc.corpus, tolower)
meta(doc.corpus[[1]])
doc.corpus <- tm_map(doc.corpus, PlainTextDocument)
meta(doc.corpus[[1]])
# http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/
rm(list=ls())
library(tm)
library(SnowballC)
library(slam)
library(reshape2)
library(ggplot2)
TEXTFILE = "d:/data/pg100.txt"
if (!file.exists(TEXTFILE)) {
download.file("http://www.gutenberg.org/cache/epub/100/pg100.txt", destfile = TEXTFILE)
}
shakespeare = readLines(TEXTFILE)
length(shakespeare)
head(shakespeare)
# There seems to be some header and footer text. We will want to get rid of that!
# Using a text editor I checked to see how many lines were occupied with metadata
# and then removed them before concatenating all of the lines into a single long, long, long string.
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
nchar(shakespeare)
# While I had the text open in the editor I noticed that sections in the
# document were separated by the following text:
# <<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM
# SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS
# PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE
# WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE
# DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS
# PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED
# COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY
# SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>
# Obviously that is going to taint the analysis. But it also serves as a
# convenient marker to divide that long, long, long string into separate documents.
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# This left me with a list of 218 documents. On further inspection,
# some of them appeared to be a little on the short side (in my limited experience,
# the bard is not known for brevity). As it turns out, the short documents were
# the dramatis personae for his plays. I removed them as well.
(dramatis.personae <- grep("Dramatis Personae", shakespeare, ignore.case = TRUE))
shakespeare = shakespeare[-dramatis.personae]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# Down to 182 documents, each of which is a complete work.
# The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(shakespeare)
doc.corpus <- Corpus(doc.vec)
inspect(doc.corpus)
length(doc.corpus)
str(doc.corpus)
summary(doc.corpus)
meta(doc.corpus[[1]])
# A corpus with 182 text documents
# The metadata consists of 2 tag-value pairs and a data frame
# Available tags are:
#   create_date creator
# Available variables in the data frame are:
#   MetaID
# There is a lot of information in those documents which is not particularly
# useful for text mining. So before proceeding any further, we will clean
# things up a bit. First we convert all of the text to lowercase and then
# remove punctuation, numbers and common English stopwords. Possibly the
# list of English stop words is not entirely appropriate for Shakespearean
# English, but it is a reasonable starting point.
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
# apparently tm used to convert tolower etc as text, but it changed so need to make PlainTextDocument
doc.corpus <- tm_map(doc.corpus, PlainTextDocument)
for (n in 1:length(doc.corpus)) {
meta(doc.corpus[[n]], tag='creator') <- 'Will Shakespeare'
meta(doc.corpus[[n]], tag='heading') <- n
}
meta(doc.corpus[[1]])
for (n in 1:length(doc.corpus)) {
meta(doc.corpus[[n]], tag='creator') <- 'Will Shakespeare'
meta(doc.corpus[[n]], tag='heading') <- n
meta(doc.corpus[[n]], tag='id') <- n
}
meta(doc.corpus[[1]])
str(doc.corpus)
for (n in 1:length(doc.corpus)) {
meta(doc.corpus[[n]], tag='creator') <- 'Will Shakespeare'
meta(doc.corpus[[n]], tag='heading') <- n
meta(doc.corpus[[n]], tag='id') <- n
meta(doc.corpus[[n]], tag='description') <- n
}
meta(doc.corpus[[6]])
# Next we perform stemming, which removes affixes from words (so, for example,
# "run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
# All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
# If we have a look at what’s left, we find that it’s just the lowercase, stripped down version
# of the text (which I have truncated here)
inspect(doc.corpus[8])
# This is where things start to get interesting. Next we create a Term Document Matrix (TDM)
# which reflects the number of times each word in the corpus is found in each of the documents.
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
# The extract from the TDM shows, for example, that the word “abandond” occurred
# once in document number 2 but was not present in any of the other first ten documents.
# We could have generated the transpose of the DTM as well.
DTM <- DocumentTermMatrix(doc.corpus)
inspect(DTM[1:10,1:10])
# Which of these proves to be most convenient will depend on the relative number
# of documents and terms in your data.
# Now we can start asking questions like: what are the most frequently occurring terms?
findFreqTerms(TDM, 2000)
# Each of these words occurred more that 2000 times.
# What about associations between words? Let’s have a look at what other words had a high association with “love”.
findAssocs(TDM, "love", 0.8)
# Well that’s not too surprising!
# note it was surprising because we didnt get what they did:
# beauti    eye
# 0.83   0.80
# From our first look at the TDM we know that there are many terms which do not
# occur very often. It might make sense to simply remove these sparse terms from the analysis.
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
dim(TDM.common)
# From the 18651 terms that we started with, we are now left with a TDM which considers
# on 61 commonly occurring terms.
inspect(TDM.common[1:10,1:10])
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
# And finally generate the visualisation.
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = log10(count))) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF") +
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
length(shakespeare)
shakespeare = readLines(TEXTFILE)
length(shakespeare)
head(shakespeare)
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
nchar(shakespeare)
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# This left me with a list of 218 documents. On further inspection,
# some of them appeared to be a little on the short side (in my limited experience,
# the bard is not known for brevity). As it turns out, the short documents were
# the dramatis personae for his plays. I removed them as well.
(dramatis.personae <- grep("Dramatis Personae", shakespeare, ignore.case = TRUE))
shakespeare = shakespeare[-dramatis.personae]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# Down to 182 documents, each of which is a complete work.
# The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(shakespeare)
doc.corpus <- Corpus(doc.vec)
inspect(doc.corpus)
length(doc.corpus)
str(doc.corpus)
summary(doc.corpus)
meta(doc.corpus[[1]])
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
# apparently tm used to convert tolower etc as text, but it changed so need to make PlainTextDocument
doc.corpus <- tm_map(doc.corpus, PlainTextDocument)
for (n in 1:length(doc.corpus)) {
meta(doc.corpus[[n]], tag='creator') <- 'Will Shakespeare'
meta(doc.corpus[[n]], tag='heading') <- n
meta(doc.corpus[[n]], tag='id') <- n
meta(doc.corpus[[n]], tag='description') <- n
}
meta(doc.corpus[[6]])
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
# All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
# If we have a look at what’s left, we find that it’s just the lowercase, stripped down version
# of the text (which I have truncated here)
inspect(doc.corpus[8])
# This is where things start to get interesting. Next we create a Term Document Matrix (TDM)
# which reflects the number of times each word in the corpus is found in each of the documents.
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
DTM <- DocumentTermMatrix(doc.corpus)
inspect(DTM[1:10,1:10])
findFreqTerms(TDM, 2000)
findAssocs(TDM, "love", 0.8)
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
dim(TDM.common)
TDM.common
nspect(TDM.common[1:10,1:10])
inspect(TDM.common[1:10,1:10])
TDM.dense <- as.matrix(TDM.common)
TDM.dense
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
findAssocs(TDM, "king", 0.8)
findAssocs(TDM, "lord", 0.8)
# http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/
rm(list=ls())
library(tm)
library(SnowballC)
library(slam)
library(reshape2)
library(ggplot2)
TEXTFILE = "d:/data/pg100.txt"
if (!file.exists(TEXTFILE)) {
download.file("http://www.gutenberg.org/cache/epub/100/pg100.txt", destfile = TEXTFILE)
}
shakespeare = readLines(TEXTFILE)
length(shakespeare)
head(shakespeare)
# There seems to be some header and footer text. We will want to get rid of that!
# Using a text editor I checked to see how many lines were occupied with metadata
# and then removed them before concatenating all of the lines into a single long, long, long string.
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
nchar(shakespeare)
# While I had the text open in the editor I noticed that sections in the
# document were separated by the following text:
# <<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM
# SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS
# PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE
# WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE
# DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS
# PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED
# COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY
# SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>
# Obviously that is going to taint the analysis. But it also serves as a
# convenient marker to divide that long, long, long string into separate documents.
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# This left me with a list of 218 documents. On further inspection,
# some of them appeared to be a little on the short side (in my limited experience,
# the bard is not known for brevity). As it turns out, the short documents were
# the dramatis personae for his plays. I removed them as well.
(dramatis.personae <- grep("Dramatis Personae", shakespeare, ignore.case = TRUE))
shakespeare = shakespeare[-dramatis.personae]
length(shakespeare)
str(shakespeare)
summary(shakespeare)
# Down to 182 documents, each of which is a complete work.
# The next task was to convert these documents into a corpus.
library(tm)
doc.vec <- VectorSource(shakespeare)
doc.corpus <- Corpus(doc.vec)
inspect(doc.corpus)
length(doc.corpus)
str(doc.corpus)
summary(doc.corpus)
meta(doc.corpus[[1]])
# A corpus with 182 text documents
# The metadata consists of 2 tag-value pairs and a data frame
# Available tags are:
#   create_date creator
# Available variables in the data frame are:
#   MetaID
# There is a lot of information in those documents which is not particularly
# useful for text mining. So before proceeding any further, we will clean
# things up a bit. First we convert all of the text to lowercase and then
# remove punctuation, numbers and common English stopwords. Possibly the
# list of English stop words is not entirely appropriate for Shakespearean
# English, but it is a reasonable starting point.
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
# apparently tm used to convert tolower etc as text, but it changed so need to make PlainTextDocument
doc.corpus <- tm_map(doc.corpus, PlainTextDocument)
############
# I have added some meta data so that the different documents are identified
# PlainTextDocument killed the metadata
#
for (n in 1:length(doc.corpus)) {
meta(doc.corpus[[n]], tag='creator') <- 'Will Shakespeare'
meta(doc.corpus[[n]], tag='heading') <- n
meta(doc.corpus[[n]], tag='id') <- n
meta(doc.corpus[[n]], tag='description') <- n
}
meta(doc.corpus[[6]])
# Next we perform stemming, which removes affixes from words (so, for example,
# "run”, “runs” and “running” all become “run”).
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument)
# All of these transformations have resulted in a lot of whitespace, which is then removed.
doc.corpus <- tm_map(doc.corpus, stripWhitespace)
# If we have a look at what’s left, we find that it’s just the lowercase, stripped down version
# of the text (which I have truncated here)
inspect(doc.corpus[8])
# This is where things start to get interesting. Next we create a Term Document Matrix (TDM)
# which reflects the number of times each word in the corpus is found in each of the documents.
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
# The extract from the TDM shows, for example, that the word “abandond” occurred
# once in document number 2 but was not present in any of the other first ten documents.
# We could have generated the transpose of the DTM as well.
DTM <- DocumentTermMatrix(doc.corpus)
inspect(DTM[1:10,1:10])
# Which of these proves to be most convenient will depend on the relative number
# of documents and terms in your data.
# Now we can start asking questions like: what are the most frequently occurring terms?
findFreqTerms(TDM, 2000)
# Each of these words occurred more that 2000 times.
# What about associations between words? Let’s have a look at what other words had a high association with “love”.
findAssocs(TDM, "love", 0.8)
findAssocs(TDM, "lord", 0.8)
findAssocs(TDM, "love", 0.8)
findAssocs(TDM, "lord", 0.8)
findAssocs(TDM, "king", 0.8)
findAssocs(TDM, "king")
findAssocs(TDM, "king", 1)
findAssocs(TDM, "king", 0.1)
findAssocs(TDM, "king", 0.6)
findAssocs(TDM, "exeunt", 0.6)
findAssocs(TDM, "exeunt", 0.8)
findAssocsocs(TDM, "love", 0.7)
findAssocs(TDM, "love", 0.7)
findAssocs(TDM, "love", 0.75)
findAssocs(TDM, "love", 0.79)
rm(list=ls())
